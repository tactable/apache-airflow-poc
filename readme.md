# üöÄ Airflow POC for Student Data Processing

##  Overview
This Proof of Concept (POC) demonstrates an **Apache Airflow** pipeline that processes student data using **CSV files**, **a Java JAR file**, and **Dockerized Airflow setup**. The pipeline extracts student data, enriches it using a **Java application**, and stores the results in **JSON format**.

---

##  High-Level Architecture
The system consists of the following key components:

1. **Apache Airflow DAG (`dummyDag.py`)**
   - Extracts student data from CSV files.
   - Calls a **Java JAR** file to enrich student data.
   - Saves the enriched data to a JSON file.

2. **Java JAR (`refdata-1.0-SNAPSHOT.jar`)**
   - Fetches additional student information (e.g., **Address, Major**) based on the Student ID.

3. **Dockerized Environment**
   - Uses a **Dockerfile** to set up Airflow with Java 8 support.
   - `docker-compose.yml` orchestrates the Airflow services.

---

## Ô∏è DAG Workflow

The DAG processes student data through the following tasks:

- **merge_csv**: Reads two CSV source files and merges them into one, renaming columns.
- **extract_student_ids**: Extracts student IDs from the merged CSV file.
- **Data_enrichment**: Calls a Java JAR to enrich student data based on student IDs.
- **save_json**: Saves the enriched student data to a JSON file.

**DAG Flow**:

```
merge_csv ‚û°Ô∏è extract_student_ids ‚û°Ô∏è Data_enrichment ‚û°Ô∏è save_json
```
![Image](https://github.com/user-attachments/assets/8c254cf9-384f-4ad6-bcd4-d63569b6f789)
---

üìå **How to Run the Airflow POC**

## 1Ô∏è‚É£ Start Airflow with devcontainer

```bash
docker-compose up -d   
``` 
<!-- Install devcontainer plugin on VScode
![Image](https://github.com/user-attachments/assets/882c9cd8-2049-4b6f-8f59-9f57c7714a42)

Search command: win+shift+p - rebuild And Reopen In Container
```bash
@command:remote-containers.rebuildAndReopenInContainer
```
![Image](https://github.com/user-attachments/assets/e738b49c-3148-4dbd-ae9f-113aed68b348) -->

## 2Ô∏è‚É£ Access the Airflow Web UI  

Open your browser and navigate to:

```bash
http://localhost:8080
```

- **Username:** `airflow`
- **Password:** `airflow`


## 3Ô∏è‚É£ Trigger the DAG from the Airflow Web UI  

1. In the **Airflow UI**, go to the **DAGs** list.  
2. Locate `TD_pipeline_POC` (or your DAG name). ![Image](https://github.com/user-attachments/assets/99deb574-95b2-45cb-91c7-311000471ab1)
3. Click the **Trigger DAG** button ‚ñ∂Ô∏è.![Image](https://github.com/user-attachments/assets/34a8af78-c256-4949-9cba-9153e3dc9b16)


## 4Ô∏è‚É£ Verify the Output  

Once the DAG has successfully run:

- Check the **output JSON file** inside your **project's `output/` folder** (since it is mounted in Docker):

```bash
cat output/enriched_students.json
```

- Alternatively, check from your **file explorer** (e.g., VS Code, Finder, Windows Explorer).

---

## ** Expected Output**
After running the DAG, the JSON file `/output/enriched_students.json` should contain:
```json
[
  {
    "studentID": "1",
    "Name": "Alice",
    "Age": 27,
    "Address": "123 Main St",
    "Major": "Computer Science"
  },
  {
    "studentID": "2",
    "Name": "Bob",
    "Age": 30,
    "Address": "456 Queen St",
    "Major": "Mathematics"
  }
]
```
---

## ‚úÖ Airflow Feature Evaluation


| Requirement                                      | Airflow Support                                    | Workaround / Notes                           |
|--------------------------------------------------|----------------------------------------------------|-----------------------------------------------------|
| **Batch & Streaming Support**      | ‚ö†Ô∏è Partial: ‚úÖ Batch, ‚ùå Limited Streaming           | Use **Apache Kafka + NIFI/Flink** for streaming.  |
| **Configurable Transformations**         | ‚úÖ Yes (via code)                                  | Use external **config files or Airflow Variables**.|
| **Horizontal Scalability**         | ‚úÖ Yes                                             | Supports CeleryExecutor and KubernetesExecutor for distributed execution across multiple workers.      |
| **Self-Hosted Deployment**                             | ‚úÖ Yes                                             | Airflow can be deployed on on-premise servers, private cloud, or Kubernetes.                                               |
| **Real-Time Stream Processing (<1 sec latency)**     | ‚ùå No                                              | Use **Apache Kafka + NIFI/Flink** for streaming.            |
| **End-to-End Testability**        | ‚ö†Ô∏è Partial                                        | Integrate testing tools like **pytest-airflow**.   |
| **Retry Mechanism**                          | ‚úÖ Yes                                             | Airflow supports automatic retries, exponential backoff, and custom error handling via retries, retry_delay, and retry_exponential_backoff.                                                |
| **Re-Runnable Workflows**                        | ‚úÖ Yes                                             | DAGs support rerunning only failed tasks via UI, CLI, API, or depends_on_past.                                               |

